# -*- coding: utf-8 -*-
"""Group_92_CAI_Assignment2 (6).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OVXJeg7BvXbqGetE5GKVfzSAguGY32Ns

**Assignment 2 - RAG Chatbot**

*   GANESH SINGH SHEKHAWAT - 2023aa05509
*   SAURABH SHARMA - 2023aa05626
*   HEENA SHRIMALI - 2023aa05349
*   AMIT KUMAR SINGH - 2022ac05376

**1. Data Collection & Preprocessing**
*   Download Apple's Financial Statements:
"""

import requests

# URLs of Apple's 2023 and 2024 Annual Reports
urls = {
    '2023': 'https://s2.q4cdn.com/470004039/files/doc_earnings/2023/q4/filing/_10-K-Q4-2023-As-Filed.pdf',
    '2024': 'https://s2.q4cdn.com/470004039/files/doc_earnings/2024/q4/filing/10-Q4-2024-As-Filed.pdf'
}

# Function to download PDFs
def download_pdf(url, filename):
    response = requests.get(url)
    if response.status_code == 200:
        with open(filename, 'wb') as f:
            f.write(response.content)
        print(f"Downloaded {filename}")
    else:
        print(f"Failed to download {filename}")

# Download the reports
for year, url in urls.items():
    download_pdf(url, f'apple_{year}_annual_report.pdf')

"""Extract Text from PDFs:"""

!pip install pymupdf

import fitz  # PyMuPDF

# Function to extract text from a PDF
def extract_text_from_pdf(pdf_path):
    document = fitz.open(pdf_path)
    text = ""
    for page_num in range(len(document)):
        page = document.load_page(page_num)
        text += page.get_text()
    return text

# Extract text from each annual report
texts = {}
for year in urls.keys():
    pdf_path = f'apple_{year}_annual_report.pdf'
    texts[year] = extract_text_from_pdf(pdf_path)
    print(f"Extracted text from {pdf_path}")

"""Clean and Structure the Data:"""

import re

# Function to clean and split text into sections
def clean_and_split_text(text):
    # Remove multiple newlines and excessive whitespace
    text = re.sub(r'\n+', '\n', text)
    text = re.sub(r'\s+', ' ', text)
    # Split text into sections based on headings or other delimiters
    sections = re.split(r'\n\s*\n', text)
    return sections

# Clean and split texts
structured_texts = {}
for year, text in texts.items():
    structured_texts[year] = clean_and_split_text(text)
    print(f"Structured text for {year}")

"""**Basic RAG Implementation**
*   Convert Financial Documents into Text Chunks:



"""

# Function to chunk text sections into smaller pieces
def chunk_text(sections, chunk_size=500):
    chunks = []
    for section in sections:
        words = section.split()
        for i in range(0, len(words), chunk_size):
            chunk = ' '.join(words[i:i + chunk_size])
            chunks.append(chunk)
    return chunks

# Chunk the structured texts
text_chunks = {}
for year, sections in structured_texts.items():
    text_chunks[year] = chunk_text(sections)
    print(f"Created {len(text_chunks[year])} chunks for {year}")

"""Embed Using a Pre-trained Model:"""

from sentence_transformers import SentenceTransformer

# Load the pre-trained embedding model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Function to embed text chunks
def embed_chunks(chunks):
    embeddings = model.encode(chunks, show_progress_bar=True)
    return embeddings

# Embed chunks for each year
embeddings = {}
for year, chunks in text_chunks.items():
    embeddings[year] = embed_chunks(chunks)
    print(f"Generated embeddings for {year}")



"""Store and Retrieve Using a Basic Vector Database:"""

!pip install faiss-cpu
import faiss
import numpy as np

# Function to create a FAISS index
def create_faiss_index(embedding_dim):
    index = faiss.IndexFlatL2(embedding_dim)
    return index

# Create a FAISS index and add embeddings
embedding_dim = embeddings['2023'].shape[1]  # Assuming all embeddings have the same dimension
index = create_faiss_index(embedding_dim)

# Add embeddings to the index
all_embeddings = []
for year in embeddings.keys():
    all_embeddings.extend(embeddings[year])
all_embeddings = np.array(all_embeddings).astype('float32')
index.add(all_embeddings)
print(f"Added {index.ntotal} embeddings to the FAISS index")



"""Implement a Retrieval Function:"""

# Function to retrieve top-k similar chunks for a query
def retrieve_similar_chunks(query, index, chunks, k=5):
    query_embedding = model.encode([query])
    distances, indices = index.search(np.array(query_embedding).astype('float32'), k)
    results = [chunks[idx] for idx in indices[0]]
    return results

# Combine all chunks for retrieval
all_chunks = []
for year in text_chunks.keys():
    all_chunks.extend(text_chunks[year])

# Example query
query = "What were Apple's net sales in 2024?"
top_k_chunks = retrieve_similar_chunks(query, index, all_chunks)
for i, chunk in enumerate(top_k_chunks, 1):
    print(f"Result {i}:\n{chunk}\n")



!pip install rank_bm25
from rank_bm25 import BM25Okapi

bm25_index = BM25Okapi(all_chunks)  # Using the same 'all_chunks' from before

import random

 #Filters the model's output to avoid misleading or hallucinated responses.
def validate_output(response):
    #  check  hallucinatory patterns
    hallucinated_patterns = ["I think", "Maybe", "It seems", "Could be", "Not sure"]

    #  uncertainty or hallucination  reject
    if any(pattern in response for pattern in hallucinated_patterns):
        return False, "The response seems uncertain or speculative. Please rephrase your query or try again."

    # Check if reponse not right . For Now Simulate
    if random.random() < 0.1:  # Randomly simulate a "bad" answer
        return False, "The response might not be accurate. Please verify it with trusted sources."

    return True, None  # The response is valid

import numpy as np

#Merge chunks by considering content overlap and relevance.
def merge_chunks(embedding_results, bm25_results, k=5):

    merged_results = list(set(embedding_results + bm25_results))

    # Prioritize chunks based on overlap, length, and relevance
    ranked_results = []
    for chunk in merged_results:
        embedding_overlap = sum(1 for e_chunk in embedding_results if e_chunk in chunk)
        bm25_overlap = sum(1 for b_chunk in bm25_results if b_chunk in chunk)
        score = embedding_overlap + bm25_overlap
        ranked_results.append((chunk, score))

    # Sort
    ranked_results.sort(key=lambda x: x[1], reverse=True)
    return [result[0] for result in ranked_results[:k]]


#Dynamically adjust weights on length
def adaptive_weight(query, k=5):
    query_length = len(query.split())
    if query_length < 5:
        #  short queries more BM25
        return 0.3, 0.7
    elif query_length > 20:
        #  long queries favour embeddings
        return 0.7, 0.3
    else:
        # medium-length keep weights balanced
        return 0.5, 0.5


#Retrieve top-k relevant chunks using both embedding-based and BM25-based methods, with adaptive re-ranking."""
def retrieve_similar_chunks_adv(query, index, chunks, bm25_index, k=5):

    # Embedding-based retrieval
    query_embedding = model.encode([query])
    distances, indices = index.search(np.array(query_embedding).astype('float32'), k * 2)
    embedding_results = [chunks[idx] for idx in indices[0]]

    # BM25-based retrieval
    bm25_scores = bm25_index.get_scores(query.split())
    bm25_indices = np.argsort(bm25_scores)[::-1][:k * 2]
    bm25_results = [chunks[idx] for idx in bm25_indices]

    # Merge chunks intelligently
    merged_results = merge_chunks(embedding_results, bm25_results, k)

    # Dynamically adjust the weights for re-ranking
    embedding_weight, bm25_weight = adaptive_weight(query, k)

    # Re-rank results based on adaptive weights
    final_scores = []
    for result in merged_results:
        embedding_score = -distances[0][embedding_results.index(result)] if result in embedding_results else 0  # Negate distance for higher similarity
        bm25_score = bm25_scores[chunks.index(result)] if result in bm25_results else 0
        final_score = embedding_weight * embedding_score + bm25_weight * bm25_score
        final_scores.append(final_score)

    # Sort by final scores and retrieve top-k results
    final_indices = np.argsort(final_scores)[::-1][:k]
    final_results = [merged_results[idx] for idx in final_indices]

    # Display the results
    print(f"Query: {query}")
    for i, result in enumerate(final_results, 1):
            output_valid, output_message = validate_output(final_results)
            if not output_valid:
                return output_message
            print(f"Result {i}:\n{result}\n")

    return final_results



query = "What were Apple's net sales in 2024?"
top_k_chunks = retrieve_similar_chunks_adv(query, index, all_chunks, bm25_index)

#UI Implementation using ipywidgets
import re

import ipywidgets as widgets
from IPython.display import display

def validate_query(query):
    # Check  queries
    if not query or query.isspace():
        return False, "Query cannot be empty."

    # profanity check
    profanity_filter =['violence', 'hate', 'explicit', 'offensive']
    if any(word in query.lower() for word in profanity_filter):
        return False, "Query contains inappropriate language."

    # Check for sensitive topics
    sensitive_topics = ["religion", "politics", "sexuality"]
    if any(topic in query.lower() for topic in sensitive_topics):
        return False, "Query touches upon sensitive topics."

    return True, None  # Query is valid

#Query UI Element
#query = widgets.Text(value="", description="Input:")
query = widgets.Text(value="", description="Input:", layout=widgets.Layout(width="50%"))
#Button UI Element
button = widgets.Button(description="Submit", button_style="primary")
resetBtn = widgets.Button(description="Reset", button_style="warning")

#Output UI Element
output = widgets.Output()  # Create an output widget

#Fetch Query Value entered in ChatBot
#query = query.value

# Align elements horizontally
button_layout = widgets.HBox([button, resetBtn])
app_layout = widgets.VBox([query, button_layout, output])

display(app_layout)


# Function to validate the input when the submit button is clicked
def on_validate_click(b):
    user_query = query.value  # Get user input
    print(user_query)
    is_valid, message = validate_query(user_query)  # Validate query

    with output:
        output.clear_output()  # Clear previous messages
        #print(f"Validation Result: {message}")  # Display the validation result
    if is_valid:
        #  retrieve results
        top_k_chunks = retrieve_similar_chunks_adv(user_query, index, all_chunks, bm25_index)
    else:
        # Handle invalid query
        print(f"Invalid query: {message}")


def reset_output(b):
  with output:
          output.clear_output()  # Clear previous messages

# Attach function to button
button.on_click(on_validate_click)
resetBtn.on_click(reset_output)

import streamlit as st

def validate_input(user_input):
    """Validation logic for user input."""
    if not user_input:
        return False, "Input cannot be empty. Please enter some text."
    elif len(user_input) < 5:
        return False, "Input must be at least 5 characters long."
    return True, "Valid input! Your message is accepted."

# Streamlit UI
st.title("RAG Financial Chatbot")

# Text Input Box
user_input = st.text_input("Enter a message:")

# Submit Button
if st.button("Submit"):
    is_valid, message = validate_input(user_input)

    # Show custom messages
    if is_valid:
        st.success(message)
    else:
        st.error(message)